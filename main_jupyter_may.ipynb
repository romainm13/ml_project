{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports de librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/may/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/may/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 6): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\n",
      "  Referenced from: /Users/may/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/may/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      " in /Users/may/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
       "      <td>A little girl climbing the stairs to her playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
       "      <td>A little girl in a pink dress going into a wooden cabin .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         image  \\\n",
       "0  1000268201_693b08cb0e.jpg#0   \n",
       "1  1000268201_693b08cb0e.jpg#1   \n",
       "2  1000268201_693b08cb0e.jpg#2   \n",
       "3  1000268201_693b08cb0e.jpg#3   \n",
       "4  1000268201_693b08cb0e.jpg#4   \n",
       "\n",
       "                                                                    caption  \n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .  \n",
       "1                                     A girl going into a wooden building .  \n",
       "2                          A little girl climbing into a wooden playhouse .  \n",
       "3                      A little girl climbing the stairs to her playhouse .  \n",
       "4                 A little girl in a pink dress going into a wooden cabin .  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Flickr8k_text/Flickr8k.token.txt\", sep='\\t', header=None)\n",
    "df.rename(columns={0:'image', 1:'caption'}, inplace=True)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df:  40460\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of df: \", len(df['image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a wooden cabin .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .  \n",
       "1                                     A girl going into a wooden building .  \n",
       "2                          A little girl climbing into a wooden playhouse .  \n",
       "3                      A little girl climbing the stairs to her playhouse .  \n",
       "4                 A little girl in a pink dress going into a wooden cabin .  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique images:  8092\n",
      "Number of images in Flicker8k_Dataset:  8091\n",
      "2387197355_237f6f41ee.jpg\n",
      "2609847254_0ec40c1cce.jpg\n",
      "2046222127_a6f300e202.jpg\n",
      "2853743795_e90ebc669d.jpg\n",
      "2696951725_e0ae54f6da.jpg\n"
     ]
    }
   ],
   "source": [
    "# Remove #digit from image name\n",
    "df['image'] = df['image'].apply(lambda x : x.split('.jpg')[0] + '.jpg')\n",
    "display(df.head())\n",
    "print(\"Number of unique images: \", len(df['image'].unique()))\n",
    "\n",
    "# Get all images' names in Flicker8k_Dataset\n",
    "img_names = glob.glob(\"Flicker8k_Dataset/*.jpg\")\n",
    "print(\"Number of images in Flicker8k_Dataset: \", len(img_names))\n",
    "\n",
    "img_names = [os.path.basename(img_name) for img_name in img_names]\n",
    "for i in range(5):\n",
    "    print(img_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df:  40460\n",
      "Length of df:  40455\n"
     ]
    }
   ],
   "source": [
    "# Remove lines in df that are not in Flicker8k_Dataset\n",
    "print(\"Length of df: \", len(df))\n",
    "df = df[df['image'].isin(img_names)]\n",
    "print(\"Length of df: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "      <td>[&lt;start&gt;, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;start&gt;, girl, going, into, wooden, building, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \\\n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .   \n",
       "1                                     A girl going into a wooden building .   \n",
       "\n",
       "                                                                                                                                                                                                                               cleaned_caption  \n",
       "0                       [<start>, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]  \n",
       "1  [<start>, girl, going, into, wooden, building, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Preprocessing -> Remove Single Character and non alpha Words. \n",
    "# Add , and tokens. token is appended such that length in max_seq_len \n",
    "# (maximum length across all captions which is 33 in our case)\n",
    "\n",
    "def remove_single_char_word(word_list):\n",
    "    lst = []\n",
    "    for word in word_list:\n",
    "        if len(word)>1:\n",
    "            lst.append(word)\n",
    "\n",
    "    return lst\n",
    "\n",
    "df['cleaned_caption'] = df['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\n",
    "df['cleaned_caption'] = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))\n",
    "\n",
    "df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\n",
    "max_seq_len = df['seq_len'].max()\n",
    "print(max_seq_len)\n",
    "\n",
    "df.drop(['seq_len'], axis = 1, inplace = True)\n",
    "df['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8360\n",
      "['<pad>', '<start>', '<end>', 'in', 'the', 'on', 'is', 'and', 'dog', 'with', 'man', 'of', 'two', 'white', 'black']\n"
     ]
    }
   ],
   "source": [
    "# %% ## Create Vocab and mapping of token to ID\n",
    "\n",
    "word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\n",
    "word_dict = Counter(word_list)\n",
    "word_dict =  sorted(word_dict, key=word_dict.get, reverse=True)\n",
    "\n",
    "print(len(word_dict))\n",
    "print(word_dict[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8360\n",
      "8360 8360\n"
     ]
    }
   ],
   "source": [
    "# ### Vocab size is 8360\n",
    "\n",
    "vocab_size = len(word_dict)\n",
    "print(vocab_size)\n",
    "\n",
    "index_to_word = {index: word for index, word in enumerate(word_dict)}\n",
    "word_to_index = {word: index for index, word in enumerate(word_dict)}\n",
    "print(len(index_to_word), len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_caption</th>\n",
       "      <th>text_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set of stairs in an entry way .</td>\n",
       "      <td>[&lt;start&gt;, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>[1, 41, 3, 89, 168, 6, 118, 52, 392, 11, 389, 3, 27, 5075, 690, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>[&lt;start&gt;, girl, going, into, wooden, building, &lt;end&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>[1, 18, 311, 63, 192, 116, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                                                    caption  \\\n",
       "0  A child in a pink dress is climbing up a set of stairs in an entry way .   \n",
       "1                                     A girl going into a wooden building .   \n",
       "\n",
       "                                                                                                                                                                                                                               cleaned_caption  \\\n",
       "0                       [<start>, child, in, pink, dress, is, climbing, up, set, of, stairs, in, an, entry, way, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]   \n",
       "1  [<start>, girl, going, into, wooden, building, <end>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]   \n",
       "\n",
       "                                                                                                                text_seq  \n",
       "0  [1, 41, 3, 89, 168, 6, 118, 52, 392, 11, 389, 3, 27, 5075, 690, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1            [1, 18, 311, 63, 192, 116, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% ### Covert sequence of tokens to IDs\n",
    "df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36409 7282\n",
      "4046 810\n"
     ]
    }
   ],
   "source": [
    "# %% ## Split In Train and validation data. Same Image should not be present in both training and validation data \n",
    "df = df.sort_values(by = 'image')\n",
    "train = df.iloc[:int(0.9*len(df))]\n",
    "valid = df.iloc[int(0.9*len(df)):]\n",
    "\n",
    "print(len(train), train['image'].nunique())\n",
    "print(len(valid), valid['image'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36409\n",
      "7282 810\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "train_samples = len(train)\n",
    "print(train_samples)\n",
    "\n",
    "unq_train_imgs = train[['image']].drop_duplicates()\n",
    "unq_valid_imgs = valid[['image']].drop_duplicates()\n",
    "print(len(unq_train_imgs), len(unq_valid_imgs))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractImageFeatureResNetDataSet():\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize a tensor image with mean and standard deviation. This transform does not support PIL Image. \n",
    "    Given mean: (mean[1],...,mean[n]) and std: (std[1],..,std[n]) for n channels, \n",
    "    this transform will normalize each channel of the input torch.*Tensor i.e., output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "        self.scaler = transforms.Resize([224, 224])\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    def __len__(self):  \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        img_loc = 'Flicker8k_Dataset/'+ str(image_name)\n",
    "\n",
    "        img = Image.open(img_loc)\n",
    "        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n",
    "\n",
    "        return image_name, t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\n",
    "train_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)\n",
    "\n",
    "valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\n",
    "valid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)\n",
    "\n",
    "resnet18 = torchvision.models.resnet18(progress=True).to(device)\n",
    "resnet18.eval()\n",
    "list(resnet18._modules)\n",
    "\n",
    "resNet18Layer4 = resnet18._modules.get('layer4').to(device)\n",
    "print(resNet18Layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7115d12194d44518112b1c45058b0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_vector(t_img):\n",
    "    t_img = Variable(t_img)\n",
    "    my_embedding = torch.zeros(1, 512, 7, 7)\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.data)\n",
    "    \n",
    "    h = resNet18Layer4.register_forward_hook(copy_data)\n",
    "    with torch.no_grad():                               # <-- no_grad context\n",
    "        resnet18(t_img)\n",
    "    \n",
    "    h.remove()\n",
    "    return my_embedding\n",
    "\n",
    "extract_imgFtr_ResNet_train = {}\n",
    "for image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    "    \n",
    "    extract_imgFtr_ResNet_train[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_train, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08b8b93f1374df8be85f3eb4bf7214c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_imgFtr_ResNet_valid = {}\n",
    "for image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n",
    "    t_img = t_img.to(device)\n",
    "    embdg = get_vector(t_img)\n",
    " \n",
    "    extract_imgFtr_ResNet_valid[image_name[0]] = embdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\n",
    "pickle.dump(extract_imgFtr_ResNet_valid, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create DataLoader which will be used to load data into Transformer Model.\n",
    "# ## FlickerDataSetResnet will return caption sequence, 1 timestep left shifted caption sequence which model will predict and Stored Image features from ResNet.\n",
    "\n",
    "class FlickerDataSetResnet():\n",
    "    def __init__(self, data, pkl_file):\n",
    "        self.data = data\n",
    "        self.encodedImgs = pd.read_pickle(pkl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        caption_seq = self.data.iloc[idx]['text_seq']\n",
    "        target_seq = caption_seq[1:]+[0]\n",
    "\n",
    "        image_name = self.data.iloc[idx]['image']\n",
    "        image_tensor = self.encodedImgs[image_name]\n",
    "        image_tensor = image_tensor.permute(0,2,3,1)\n",
    "        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n",
    "\n",
    "        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view\n",
    "\n",
    "train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\n",
    "train_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)\n",
    "\n",
    "valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\n",
    "valid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pe.size(0) < x.size(0):\n",
    "            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n",
    "        self.pe = self.pe[:x.size(0), : , : ]\n",
    "        \n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n",
    "        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n",
    "        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_size)\n",
    "        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.last_linear_layer.bias.data.zero_()\n",
    "        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def generate_Mask(self, size, decoder_inp):\n",
    "        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n",
    "\n",
    "        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n",
    "        decoder_input_pad_mask_bool = decoder_inp == 0\n",
    "\n",
    "        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n",
    "\n",
    "    def forward(self, encoded_image, decoder_inp):\n",
    "        encoded_image = encoded_image.permute(1,0,2)\n",
    "\n",
    "        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n",
    "        \n",
    "        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n",
    "        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n",
    "        \n",
    "\n",
    "        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n",
    "        decoder_input_mask = decoder_input_mask.to(device)\n",
    "        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n",
    "        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n",
    "        \n",
    "\n",
    "        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n",
    "        \n",
    "        final_output = self.last_linear_layer(decoder_output)\n",
    "\n",
    "        return final_output,  decoder_input_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5972b622b9624d838aec356e962ac464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bp/ycy9jcl91bxf4cd2qtmrjvtc0000gn/T/ipykernel_69684/2722570739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mictModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bp/ycy9jcl91bxf4cd2qtmrjvtc0000gn/T/ipykernel_69684/1048002791.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoded_image, decoder_inp)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_inp_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_input_pad_mask_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    292\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;31m# feed forward block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ##  Train the Model\n",
    "# ### The cross entropy loss has been masked at time steps where input token is <'pad'>.\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "scorebleu = []\n",
    "\n",
    "EPOCH = 30\n",
    "\n",
    "ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\n",
    "optimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\n",
    "# optimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.001) NOT WORKING AT ALL BUG\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "min_val_loss = float('Inf')\n",
    "\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    total_epoch_train_loss = 0\n",
    "    total_epoch_valid_loss = 0\n",
    "    total_train_words = 0\n",
    "    total_valid_words = 0\n",
    "    ictModel.train()\n",
    "\n",
    "    ### Train Loop\n",
    "    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_embed = image_embed.squeeze(1).to(device)\n",
    "        caption_seq = caption_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n",
    "        output = output.permute(1, 2, 0)\n",
    "\n",
    "        loss = criterion(output,target_seq)\n",
    "\n",
    "        loss_masked = torch.mul(loss, padding_mask)\n",
    "\n",
    "        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n",
    "\n",
    "        final_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n",
    "        total_train_words += torch.sum(padding_mask)\n",
    "\n",
    " \n",
    "    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n",
    "\n",
    "    ### Eval Loop\n",
    "    ictModel.eval()\n",
    "    with torch.no_grad():\n",
    "        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n",
    "\n",
    "            image_embed = image_embed.squeeze(1).to(device)\n",
    "            caption_seq = caption_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n",
    "            output = output.permute(1, 2, 0)\n",
    "\n",
    "            loss = criterion(output,target_seq)\n",
    "\n",
    "            loss_masked = torch.mul(loss, padding_mask)\n",
    "\n",
    "            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n",
    "            total_valid_words += torch.sum(padding_mask)\n",
    "\n",
    "    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n",
    "  \n",
    "    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n",
    "  \n",
    "    if min_val_loss > total_epoch_valid_loss:\n",
    "        print(\"Writing Model at epoch \", epoch)\n",
    "        torch.save(ictModel, './BestModel')\n",
    "        min_val_loss = total_epoch_valid_loss\n",
    "  \n",
    "\n",
    "    scheduler.step(total_epoch_valid_loss.item())\n",
    "\n",
    "    train_losses.append(total_epoch_train_loss.item())\n",
    "    val_losses.append(total_epoch_valid_loss.item())\n",
    "\n",
    "    # Convertir les indices prédits en séquences de mots\n",
    "    predicted_sequences = []\n",
    "    with torch.no_grad():\n",
    "        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n",
    "            image_embed = image_embed.squeeze(1).to(device)\n",
    "            caption_seq = caption_seq.to(device)\n",
    "            #ici l'output est censé être la sortie du décodeur avant passage par la couche d'embedding\n",
    "            output, _ = ictModel.forward(image_embed, caption_seq)\n",
    "            output = torch.argmax(output, dim=2).cpu().numpy().tolist()\n",
    "            predicted_sequences.extend(output)\n",
    "\n",
    "    # Convertir les indices réels en séquences de mots\n",
    "    reference_sequences = []\n",
    "    for _, target_seq, _ in valid_dataloader_resnet:\n",
    "        target_seq = target_seq.cpu().numpy().tolist()\n",
    "        reference_sequences.extend(target_seq)\n",
    "\n",
    "    # Calculer la métrique BLEU\n",
    "    bleu_score = corpus_bleu([[ref] for ref in reference_sequences], predicted_sequences)\n",
    "\n",
    "    # Afficher le score BLEU\n",
    "    scorebleu.append(bleu_score)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.plot(scorebleu, label = 'Blue score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylabel('Metrique d evaluation')\n",
    "plt.title('Evaluation metric and training and validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./BestModel')\n",
    "start_token = word_to_index['<start>']\n",
    "end_token = word_to_index['<end>']\n",
    "pad_token = word_to_index['<pad>']\n",
    "max_seq_len = 33\n",
    "print(start_token, end_token, pad_token)\n",
    "\n",
    "valid_img_embed = pd.read_pickle('EncodedImageValidResNet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(K, img_nm): \n",
    "    img_loc = '/home/romainm/ml_project/Flicker8k_Dataset/'+str(img_nm)\n",
    "    image = Image.open(img_loc).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "    model.eval() \n",
    "    valid_img_df = valid[valid['image']==img_nm]\n",
    "    print(\"Actual Caption : \")\n",
    "    print(valid_img_df['caption'].tolist())\n",
    "    img_embed = valid_img_embed[img_nm].to(device)\n",
    "\n",
    "\n",
    "    img_embed = img_embed.permute(0,2,3,1)\n",
    "    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n",
    "\n",
    "\n",
    "    input_seq = [pad_token]*max_seq_len\n",
    "    input_seq[0] = start_token\n",
    "\n",
    "    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n",
    "    predicted_sentence = []\n",
    "    with torch.no_grad():\n",
    "        for eval_iter in range(0, max_seq_len):\n",
    "\n",
    "            output, padding_mask = model.forward(img_embed, input_seq)\n",
    "\n",
    "            output = output[eval_iter, 0, :]\n",
    "\n",
    "            values = torch.topk(output, K).values.tolist()\n",
    "            indices = torch.topk(output, K).indices.tolist()\n",
    "\n",
    "            next_word_index = random.choices(indices, values, k = 1)[0]\n",
    "\n",
    "            next_word = index_to_word[next_word_index]\n",
    "\n",
    "            input_seq[:, eval_iter+1] = next_word_index\n",
    "\n",
    "\n",
    "            if next_word == '<end>' :\n",
    "                break\n",
    "\n",
    "            predicted_sentence.append(next_word)\n",
    "    print(\"\\n\")\n",
    "    print(\"Predicted caption : \")\n",
    "    print(\" \".join(predicted_sentence+['.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération des embeddings pour le dataset de test\n",
    "#métrique d'évaluation : prendre le meilleur résultat du calcul de similarité entre l'embedding prédit et les embeddings de référence\n",
    "\n",
    "for img in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(1, unq_valid_imgs.iloc[50]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(2, unq_valid_imgs.iloc[50]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(1, unq_valid_imgs.iloc[100]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(2, unq_valid_imgs.iloc[100]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7c637b33922fed443f60bb2e75e430622fea5fd92d80550769543186c8fa0ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
